{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Study  N1      Mean1        SD1  N2      Mean2       SD2  Cohen's d  \\\n",
      "0      1  71  66.599697   7.340280  34  72.592990  7.339918  -0.816507   \n",
      "1      2  43  31.029225  17.486640  22  38.878774  8.185087  -0.521928   \n",
      "2      3  40  45.212112  11.479175  52  50.835894  9.368437  -0.544089   \n",
      "3      4  61  48.318092  16.777639  79  53.598442  7.995107  -0.419397   \n",
      "4      5  83  72.997020  11.757489  22  79.398558  5.198974  -0.595486   \n",
      "\n",
      "   Hedges' g        SE  Items  Cronbach Alpha  \n",
      "0  -0.810547  0.216035     10        0.808713  \n",
      "1  -0.515690  0.266094     10        0.827274  \n",
      "2  -0.539542  0.214102     10        0.891778  \n",
      "3  -0.417114  0.172278     10        0.877135  \n",
      "4  -0.591139  0.243293     10        0.941330  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "def compute_effect_sizes(n1, mean1, sd1, n2, mean2, sd2):\n",
    "    # Pooled standard deviation\n",
    "    s_pooled = np.sqrt(((n1 - 1) * sd1**2 + (n2 - 1) * sd2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    cohen_d = (mean1 - mean2) / s_pooled\n",
    "    \n",
    "    # Hedges' g correction factor\n",
    "    correction_factor = 1 - (3 / (4 * (n1 + n2) - 9))\n",
    "    hedges_g = cohen_d * correction_factor\n",
    "    \n",
    "    # Standard error of Cohen's d\n",
    "    se_d = np.sqrt((n1 + n2) / (n1 * n2) + (cohen_d**2 / (2 * (n1 + n2))))\n",
    "    \n",
    "    return cohen_d, hedges_g, se_d\n",
    "\n",
    "def compute_cronbach_alpha():\n",
    "    # Generate a high reliability score (Cronbach's alpha typically ranges 0-1)\n",
    "    return np.random.uniform(0.8, 0.95)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "num_studies = 100\n",
    "\n",
    "data = []\n",
    "for i in range(1, num_studies + 1):\n",
    "    n1 = np.random.randint(20, 101)  # Sample size for group 1\n",
    "    n2 = np.random.randint(20, 101)  # Sample size for group 2\n",
    "    mean1 = np.random.uniform(30, 80)  # Mean for group 1\n",
    "    mean2 = mean1 + np.random.uniform(3, 8)  # Mean for group 2 (slightly higher)\n",
    "    sd1 = np.random.uniform(5, 20)  # Standard deviation for group 1\n",
    "    sd2 = np.random.uniform(5, 20)  # Standard deviation for group 2\n",
    "    \n",
    "    cohen_d, hedges_g, se_d = compute_effect_sizes(n1, mean1, sd1, n2, mean2, sd2)\n",
    "    cronbach_alpha = compute_cronbach_alpha()\n",
    "    \n",
    "    data.append([i, n1, mean1, sd1, n2, mean2, sd2, cohen_d, hedges_g, se_d, 10, cronbach_alpha])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Study\", \"N1\", \"Mean1\", \"SD1\", \"N2\", \"Mean2\", \"SD2\", \"Cohen's d\", \"Hedges' g\", \"SE\", \"Items\", \"Cronbach Alpha\"])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"meta_analysis_data.csv\", index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cronbach_alpha():\n",
    "    # Generate a high reliability score (Cronbach's alpha typically ranges 0-1)\n",
    "    return np.random.uniform(0.8, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAJA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Study  N1      Mean1        SD1  N2      Mean2        SD2  Cohen's d  \\\n",
      "0      1  71  97.535715  10.986585  72  90.215776   6.560186   0.810355   \n",
      "1      2  94  72.962445   6.428668  95  69.625358  11.508885   0.357496   \n",
      "2      3  49  60.616956   6.834045  50  58.798706   8.042422   0.243442   \n",
      "3      4  68  76.238733   5.466657  69  72.240123  14.737555   0.358748   \n",
      "4      5  81  89.258798  10.142344  82  87.262060  10.924146   0.189391   \n",
      "\n",
      "   Hedges' g        SE  Items  Cronbach Alpha  \n",
      "0   0.806037  0.173981      8        0.731199  \n",
      "1   0.356061  0.146638      8        0.711282  \n",
      "2   0.241554  0.201761      8        0.804951  \n",
      "3   0.356751  0.172245      8        0.746554  \n",
      "4   0.188508  0.157006      8        0.709290  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "def compute_effect_sizes(n1, mean1, sd1, n2, mean2, sd2):\n",
    "    # Pooled standard deviation\n",
    "    s_pooled = np.sqrt(((n1 - 1) * sd1**2 + (n2 - 1) * sd2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    cohen_d = (mean1 - mean2) / s_pooled\n",
    "    \n",
    "    # Hedges' g correction factor\n",
    "    correction_factor = 1 - (3 / (4 * (n1 + n2) - 9))\n",
    "    hedges_g = cohen_d * correction_factor\n",
    "    \n",
    "    # Standard error of Cohen's d\n",
    "    se_d = np.sqrt((n1 + n2) / (n1 * n2) + (cohen_d**2 / (2 * (n1 + n2))))\n",
    "    \n",
    "    return cohen_d, hedges_g, se_d\n",
    "\n",
    "def compute_cronbach_alpha():\n",
    "    # Generate a high reliability score (Cronbach's alpha typically ranges 0-1)\n",
    "    return np.random.uniform(0.7, 0.9)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "num_studies = 113\n",
    "\n",
    "data = []\n",
    "for i in range(1, num_studies + 1):\n",
    "    n1 = np.random.randint(20, 101)  # Sample size for group 1\n",
    "    n2 = n1 + np.random.randint(1, 2)  # Sample size for group 2\n",
    "    mean1 = np.random.uniform(50, 100)  # Mean for group 1\n",
    "    mean2 = mean1 - np.random.uniform(0, 10)  # Mean for group 2 (slightly lower)\n",
    "    sd1 = np.random.uniform(5, 15)  # Standard deviation for group 1\n",
    "    sd2 = np.random.uniform(5, 15)  # Standard deviation for group 2\n",
    "    \n",
    "    cohen_d, hedges_g, se_d = compute_effect_sizes(n1, mean1, sd1, n2, mean2, sd2)\n",
    "    cronbach_alpha = compute_cronbach_alpha()\n",
    "    \n",
    "    data.append([i, n1, mean1, sd1, n2, mean2, sd2, cohen_d, hedges_g, se_d, 8, cronbach_alpha])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Study\", \"N1\", \"Mean1\", \"SD1\", \"N2\", \"Mean2\", \"SD2\", \"Cohen's d\", \"Hedges' g\", \"SE\", \"Items\", \"Cronbach Alpha\"])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"meta_analysis_data_VAJA.csv\", index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Study  Correlation  Sample Size  Frequency Event\n",
      "0      1     0.391717          152               65\n",
      "1      2     0.377997           70               15\n",
      "2      3     0.371433          137                7\n",
      "3      4     0.660999          199               34\n",
      "4      5     0.796106          343              165\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "num_studies = 100\n",
    "\n",
    "data = []\n",
    "for i in range(1, num_studies + 1):\n",
    "    sample_size = np.random.randint(50, 500)  # Sample size for each study\n",
    "    \n",
    "    # Ensure approximately 70% positive correlations and 30% negative correlations\n",
    "    if np.random.rand() < 0.9:\n",
    "        correlation = np.random.uniform(0.3, 0.8)  # Positive correlation\n",
    "    else:\n",
    "        correlation = np.random.uniform(-0.8, -0.3)  # Negative correlation\n",
    "    \n",
    "    frequency_event = np.random.randint(5, sample_size // 2)  # Frequency of observed event\n",
    "    \n",
    "    data.append([i, correlation, sample_size, frequency_event])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Study\", \"Correlation\", \"Sample Size\", \"Frequency Event\"])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"meta_analysis_correlation_data.csv\", index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
